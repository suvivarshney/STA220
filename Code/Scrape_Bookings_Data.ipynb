{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqI8GDyZK43B",
        "outputId": "59191038-920f-48b3-e02d-382846e5846e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/mydrive\n"
          ]
        }
      ],
      "source": [
        "# Importing drive for google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mydrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9alEZdTkLpOx",
        "outputId": "d12a1ba7-0c8d-4950-843e-d1fc4c908e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requests-cache\n",
            "  Downloading requests_cache-1.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25.5 in /usr/local/lib/python3.9/dist-packages (from requests-cache) (1.26.15)\n",
            "Collecting url-normalize>=1.4\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-cache) (3.1.1)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.9/dist-packages (from requests-cache) (22.2.0)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.9/dist-packages (from requests-cache) (2.27.1)\n",
            "Collecting cattrs>=22.2\n",
            "  Downloading cattrs-22.2.0-py3-none-any.whl (35 kB)\n",
            "Collecting exceptiongroup\n",
            "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22->requests-cache) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22->requests-cache) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22->requests-cache) (2.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from url-normalize>=1.4->requests-cache) (1.16.0)\n",
            "Installing collected packages: url-normalize, exceptiongroup, cattrs, requests-cache\n",
            "Successfully installed cattrs-22.2.0 exceptiongroup-1.1.1 requests-cache-1.0.0 url-normalize-1.4.3\n"
          ]
        }
      ],
      "source": [
        "# Import os to change directory\n",
        "import os\n",
        "os.chdir(\"/content/mydrive\")\n",
        "!pip install requests-cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiwNmoEigpy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VIerQjp2QcN0"
      },
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import requests_cache\n",
        "import re\n",
        "\n",
        "# Initialize session using request_cache\n",
        "session = requests_cache.CachedSession('demo_cache_bookings')\n",
        "\n",
        "'''\n",
        "This function scrapes the Hotel details.\n",
        "Accepts 2 arguments:\n",
        "link: (str) Link to the hotel to be scrapped\n",
        "revs: (str) Reviews available for the hotel\n",
        "\n",
        "Returns a list of reviews, amenities and ratings for different categories\n",
        "'''\n",
        "def scrape_hotel(link,revs):\n",
        "  page_name = (link.split('/hotel/us/')[1]).split('.html')[0] # Get the page name from the link \n",
        "  review_list = 0\n",
        "  review_list = get_reviews(page_name,revs) # Call the funtion to get reviews \n",
        "  amenities_list_names = []\n",
        "  ratings = {}\n",
        "  time.sleep(0.1)\n",
        "  response_c = session.get(url = 'https://www.booking.com'+link) # Hit the url for the hotel using link\n",
        "  soup_details = BeautifulSoup(response_c.text, 'html.parser')\n",
        "  details_list = soup_details.findAll('div')\n",
        "  span_list = soup_details.findAll('span')\n",
        "\n",
        "  # Scrape the list of amentites from the HTML response recieved\n",
        "  for detail in details_list:\n",
        "    if (str(detail).startswith('<div data-testid=\"property-most-popular-facilities-wrapper\"')):\n",
        "      soup_amenities = BeautifulSoup(str(detail), 'html.parser')\n",
        "      amenities_list = soup_amenities.findAll('span')\n",
        "      for amenity in amenities_list:\n",
        "        if str(amenity).startswith('<span class='):\n",
        "          if ((str(amenity).split('>')[1]).split('</')[0]).startswith('<') :\n",
        "            pass\n",
        "          else :\n",
        "            amenities_list_names.append((str(amenity).split('>')[1]).split('</')[0])\n",
        "      break\n",
        "  # Scrape the best rating for the hotel\n",
        "  scripts = soup_details.findAll('script')\n",
        "  for script in scripts:\n",
        "    if str(script).startswith('<script nonce=') and 'type=\"application/ld+json\"' in str(script):\n",
        "      li = str(script).split('\\n')\n",
        "      for i in li:\n",
        "        if i.strip().startswith('\"bestRating\"'):\n",
        "          ratings['Best Rating'] = float((i.strip().split(':')[1]).split(',')[0])\n",
        "\n",
        "  # Scrape the list of rating available for each Hotel (Staff, Facilities, Comfort ...)\n",
        "  rats = ['Staff', 'Facilities', 'Cleanliness', 'Comfort', 'Value for money', 'Location', 'Free WiFi']\n",
        "  p=0\n",
        "  for num in range(0, len(span_list)):\n",
        "    if str(span_list[num]).startswith('<span class=\"c-score-bar__title\">'):\n",
        "      ratings[rats[p]] = float((str(span_list[num+1]).split('</')[0]).split('>')[1])\n",
        "      p=p+1\n",
        "  \n",
        "  # Return list of reviews, amenities and ratings for different categories\n",
        "  return ([review_list,amenities_list_names,ratings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V-j2ixuOfmbm"
      },
      "outputs": [],
      "source": [
        "# Function to get the payload for reviews url\n",
        "def set_payload(offset, rows, pagename, aid='355028', sid='f87a5cba90a64eea25b1f229ea4fd9e1'):\n",
        "    payload = {\n",
        "    'aid': aid,\n",
        "    'sid': sid,\n",
        "    'srpvid': 'a4a298caac950022',\n",
        "    'cc1': 'us',\n",
        "    'pagename': pagename,\n",
        "    'r_lang': '',\n",
        "    'review_topic_category_id': '',\n",
        "    'type': 'total',\n",
        "    'score': '',\n",
        "    'sort': '',\n",
        "    'time_of_year':'', \n",
        "    'dist': '1',\n",
        "    'offset':str(offset),\n",
        "    'rows': str(rows),\n",
        "    'rurl': '',\n",
        "    'text': '',\n",
        "    'translate':'', \n",
        "    '_': '1679348660326'\n",
        "    }\n",
        "    return payload\n",
        "\n",
        "# Function to get header for reviews url\n",
        "def set_header(cookie):\n",
        "    headers ={\n",
        "    'authority': 'www.booking.com',\n",
        "    'method': 'GET',\n",
        "    'path': '/reviewlist.en-gb.html',\n",
        "    'scheme': 'https',\n",
        "    'accept': '*/*',\n",
        "    'accept-encodingo': 'gzip, deflate, br',\n",
        "    'accept-language': 'en-GB,en;q=0.7',\n",
        "    'cookie': cookie,\n",
        "    'referer': 'https://www.booking.com/',\n",
        "    'sec-fetch-dest': 'empty',\n",
        "    'sec-fetch-mode': 'cors',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'sec-gpc': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
        "    'x-booking-aid': '355028',\n",
        "    'x-booking-client-info': 'THHSOFRURURYNYHIYTLRQJRbWdWOGVO|1,THHSOFRURURYNYHIYTLRQJRbWdWOGVO|3,THHSOFRURURYNYHIYTLRQJRbWdWOGVO|5,aaTBNZZJRLESPIDNJDPVBC|1,aaTBNZZJRLESPIDNJDPVBC|2,adUAAVfDfWZJEHSCGVbSHT|2',\n",
        "    'x-booking-csrf': '8QsZZAAAAAA=fh8oX3vXdXs-JURUAcIAth4G1ENmQTTB9Q2VI_H5vvAQuF0ZzQ_S1HwN2bIYvuXLdJiHoSqDFEoiX2OkySY_-5IuTkZnkP7VmdCseFXjpKzgubBULW6AnTyNh2HFVlrGMhYQcpIsrek0WB596M9i-U4EtJrGXV5LnBJroFMoc__Bt19r1U08DlIsNruDBOqwoS6TDw-QFN2bH_Dw',\n",
        "    'x-booking-info': '1680920,1683520,1687140,1689170,1692140,1692950,1696640,1696800,1696860,1699500,1700010,1700580,THHSOFRURURYNYHIYTLRQJRbWdWOGVO|1,THHSOFRURURYNYHIYTLRQJRbWdWOGVO|3,THHSOFRURURYNYHIYTLRQJRbWdWOGVO|5,aaTBNZZJRLESPIDNJDPVBC|1,eWHMcCcCcCFKJBKWUbPNadSFbTdNDNSNC|1,1698510|2,1700010|2,1696800|3,1680920|1,1700010|4,aaTBNZZJRLESPIDNJDPVBC|2,adUAAVfDfWZJEHSCGVbSHT|2,HINZJLeUXSaZbOTMXC|1,HINZJLeUXSaZbdKNKNKPJdBJOTXNORe|1,HINZJLeUXSaZbOTMXC|2,HINZJLeUXSaZbOTMXC|6,HINZJLeUXSaZbdKNKNKPJdBJOTXNORe|2,HINZJLeUXSaZbdKNKNKPJdBJOTXNORe|8,HINZJLeUXSaZbCcBUEXO|5,HINZJLeUXSaZbCcBUEXO|7,HINZJLeUXSaZbOTMXC|3,YdXfMTXEUDdeOYSCaIfWcACVVLZPecOEO|1,YdXfMTXEUDdeOYSCaIfWcACVVLZPecOEO|6,YdXfMTXEUDdeOYSCaIfWcACVVLZPecOEO|3,INFddKNKNKPBDJJHMVGPLTLReASdVLT|2,aXBNTfZHYHQDVCXdUFDeTQQVDaVYEO|1,aXBNTfZHYHQDVCXdUFDeTQQVDaVYEO|3',\n",
        "    'x-booking-language-code': 'en-gb',\n",
        "    'x-booking-pageview-id': '8ac998d8834b026a',\n",
        "    'x-booking-session-id': 'f87a5cba90a64eea25b1f229ea4fd9e1',\n",
        "    'x-booking-sitetype-id': '1',\n",
        "    'x-partner-channel-id': '3',\n",
        "    'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "    return headers\n",
        "\n",
        "# Function to get headers for hotels searchresults url  \n",
        "def set_header_a(cookie):\n",
        "    headers ={\n",
        "    'authority': 'www.booking.com',\n",
        "    'method': 'GET',\n",
        "    'path': '/searchresults.en-us.html',\n",
        "    'scheme': 'https',\n",
        "    'accept': 'text/html',\n",
        "    'accept-encodingo': 'gzip, deflate, br',\n",
        "    'accept-language': 'en-GB,en;q=0.7',\n",
        "    'cache-control': 'max-age=0',\n",
        "    'cookie': cookie,\n",
        "    'upgrade-insecure-requests': '1',\n",
        "    'sec-fetch-dest': 'document',\n",
        "    'sec-fetch-mode': 'navigate',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'sec-gpc': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'\n",
        "    }\n",
        "    return headers\n",
        "\n",
        "# Function to get payload for hotels searchresults url\n",
        "def set_payload_a(city, dest, aid, sid, offset):\n",
        "    payload = {\n",
        "    'aid': aid,\n",
        "    'sid': sid,\n",
        "    'ss': city,\n",
        "    'ssne': city,\n",
        "    'ssne_untouched': city,\n",
        "    'highlighted_hotels': '374407',\n",
        "    'lang': 'en-us',\n",
        "    'sb': '1',\n",
        "    'src_elem': 'sb',\n",
        "    'src': 'searchresults',\n",
        "    'dest_id': dest,\n",
        "    'dest_type': 'city',\n",
        "    'checkin': '2023-04-28',\n",
        "    'checkout': '2023-04-30',\n",
        "    'group_adults': '2',\n",
        "    'no_rooms': '1',\n",
        "    'group_children': '0',\n",
        "    'sb_travel_purpose': 'leisure',\n",
        "    'offset': str(offset)\n",
        "    }\n",
        "    return payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BlbjQyZAFUfy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This function scrapes the Hotel details.\n",
        "Accepts 2 arguments:\n",
        "page_name: (str) hotel for which reviews need to be scrapped\n",
        "revs: (str) Reviews available for the hotel\n",
        "\n",
        "Returns a list of reviews\n",
        "'''\n",
        "def get_reviews(page_name,revs):\n",
        "  url='https://www.booking.com/reviewlist.en-gb.html' # Set the reviews URL\n",
        "  dict_list=[]\n",
        "  #set the number of pages in this loop\n",
        "  revs = revs.split(' ')[0].strip()\n",
        "  revs = re.sub(',', '', revs)\n",
        "  number_of_pages = int(int(revs)/10)\n",
        "  for offset in tqdm.tqdm(range(number_of_pages)):\n",
        "    #Set offset and number of rows here\n",
        "    payload = set_payload(offset=offset*10, rows=10, pagename=page_name)\n",
        "    #Set cookie here\n",
        "    headers = set_header(cookie=\n",
        "                     'px_init=0; bkng_sso_session=e30; cors_js=1; OptanonConsent=consentCausedByGPCSignalStored=1676350216807&implicitConsentCountry=nonGDPR&implicitConsentDate=1676350578327; bkng_sso_ses=eyJib29raW5nX2dsb2JhbCI6W3siYSI6MSwiaCI6IlpaWXQzN1U2cWxmSG5zWlN6bkJmU3lacnZ0bndBN3IvZjJVd2lreGJVeEUifV19; bkng_sso_auth=CAEQARpWiuKYi1DOHJSuO4hRLUcoob8rBLZ39K9khJGFmTPO4onFDiuuzjOjOmcFBJGuerC/vcR6HYSE9ycFThhsbi9pV/XvIfCvie32L75iYUAamGzwavCmlNAiEMNjyhVSP6kdS4TMEjCNd6s=; BJS=-; _pxhd=kHWt2-Q0inN0bQMXUXG%2FeyUUriMu0j5ZkFfAi%2FqxlWm0D6ATdwCL95PtIIQHmkCBuvs8qXVDDFky6q8Trh8vMw%3D%3D%3Ao8ZNquaJYFJWHfDCrm9zH8MGa9Jell5wENexYptlUz%2FiZPvDD8MHYk%2FX--Z22Ri0A2mN5QrwCOOUaJqmvk1xhNLdJsUIeHfbKemFRhLhKRw%3D; lastSeen=0; bkng=11UmFuZG9tSVYkc2RlIyh9YXSgTtYpR%2F1WOjMvuuinviEMkxKMKaR8RszOeejweDlwtP1u52V2KKHcuZWtAZXTCazM97dd71rXbb02cdIlBFgcQWZEsiJuYMTFSTHwgEN%2F4WK8rNDgBhcNIUGP6ch8GSLQSQolvOnV8yGkdtp5trbYww1Oad21qOwR5g%2BoBEpQZHaR3ehEvUWa75BXgEUL%2Fg%3D%3D',\n",
        "                    )\n",
        "    \n",
        "    time.sleep(0.1)\n",
        "    r=session.get(url, headers=headers,params=payload) # Hit the reviews URL\n",
        "    soup=BeautifulSoup(r.text)\n",
        "    review_list = soup.find_all('li',{'class':'review_list_new_item_block'})\n",
        "    \n",
        "    #Looping through the all the reviews\n",
        "    for i in review_list: \n",
        "        try: # Scrape name\n",
        "            Reviewer_Name=i.find_all('span',{'class':'bui-avatar-block__title'})[0].string\n",
        "        except:\n",
        "            Reviewer_Name=None\n",
        "\n",
        "        try: # Scrape country\n",
        "            Reviewer_Country=i.find_all('span',{'class':'bui-avatar-block__subtitle'})[0].text.strip(' \\n')\n",
        "        except:\n",
        "            Reviewer_Country=None\n",
        "\n",
        "        try: # Scrape Room type\n",
        "            Room_Type=i.find_all('div',{'class':'bui-list__body'})[0].string.strip(' \\n')\n",
        "        except:\n",
        "            Room_Type=None\n",
        "\n",
        "        try: # Scrape Month\n",
        "            Review_Month=i.find_all('span',{'class':'c-review-block__date'})[0].text.strip(' \\n')\n",
        "        except:\n",
        "            Review_Month=None\n",
        "\n",
        "        try: # Scrape stay period\n",
        "            Stay_Period=i.find_all('div',{'class':'bui-list__body'})[1].text.split('·')[0].strip(' \\n')\n",
        "        except:\n",
        "            Stay_Period= None\n",
        "\n",
        "        try: # Scrape review group like 'Family' etc.\n",
        "            Reviewer_Group_Type=i.find_all('div',{'class':'bui-list__body'})[-1].string.strip(' \\n')\n",
        "        except:\n",
        "            Reviewer_Group_Type=None\n",
        "\n",
        "        try: # Scrape start date\n",
        "            Stay_Start_Date=i.find_all('span',{'class':'c-review-block__date'})[-1].string.strip(' \\n').split(':')[-1].strip(' ')\n",
        "        except:\n",
        "            Stay_Start_Date=None\n",
        "\n",
        "        try: # Scrape review heading\n",
        "            Main_Review=i.find_all('h3',{'class':'c-review-block__title c-review__title--ltr'})[0].string.strip(' \\n')\n",
        "        except:\n",
        "            Main_Review=None\n",
        "\n",
        "        try: # Scrape rating for review\n",
        "            Review_Rating=i.find_all('div',{'class':'bui-review-score__badge'})[0].string.strip(' \\n')\n",
        "        except:\n",
        "            Review_Rating=None\n",
        "\n",
        "        try: # Scrape review like\n",
        "            Review_Likes=i.find_all('span',{'class':'c-review__body'})[0].string\n",
        "        except:\n",
        "            Review_Likes=None\n",
        "\n",
        "        try: # Scrape review dislike\n",
        "            Review_Dislikes=i.find_all('span',{'class':'c-review__body'})[1].string\n",
        "        except:\n",
        "            Review_Dislikes=None\n",
        "        \n",
        "        # Add data into a dictionary and append to a list of reviews\n",
        "        dict_list.append({\n",
        "            'Reviewer_Name':Reviewer_Name, \n",
        "            'Reviewer_Country':Reviewer_Country,\n",
        "            'Room_Type':Room_Type,\n",
        "            'Review_Month':Review_Month,\n",
        "            'Stay_Period':Stay_Period,\n",
        "            'Stay_Start_Date':Stay_Start_Date,\n",
        "            'Reviewer_Group_Type':Reviewer_Group_Type,\n",
        "            'Main_Review':Main_Review,\n",
        "            'Review_Rating':Review_Rating,\n",
        "            'Review_Likes':Review_Likes,\n",
        "            'Review_Dislikes':Review_Dislikes,\n",
        "        })\n",
        "  return dict_list # Return list of reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3_1aLvNdu35U"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This function gets the number of properties loaded for a city\n",
        "Accepts 1 arguments:\n",
        "city_: (str) Name of the city you want the list of hotels for \n",
        "\n",
        "Returns number of properties loaded\n",
        "'''\n",
        "def get_pages(city_):\n",
        "  url = 'https://www.booking.com/searchresults.en-us.html' # Set the URL\n",
        "  payload = set_payload_a(city=city_[0], dest=city_[1], aid=city_[2], sid=city_[3], offset=25)\n",
        "  #Set your cookie here\n",
        "  headers = set_header(cookie=\n",
        "                     'px_init=0; bkng_sso_session=e30; cors_js=1; OptanonConsent=consentCausedByGPCSignalStored=1676350216807&implicitConsentCountry=nonGDPR&implicitConsentDate=1676350578327; bkng_sso_ses=eyJib29raW5nX2dsb2JhbCI6W3siYSI6MSwiaCI6IlpaWXQzN1U2cWxmSG5zWlN6bkJmU3lacnZ0bndBN3IvZjJVd2lreGJVeEUifV19; bkng_sso_auth=CAEQARpWiuKYi1DOHJSuO4hRLUcoob8rBLZ39K9khJGFmTPO4onFDiuuzjOjOmcFBJGuerC/vcR6HYSE9ycFThhsbi9pV/XvIfCvie32L75iYUAamGzwavCmlNAiEMNjyhVSP6kdS4TMEjCNd6s=; BJS=-; _pxhd=kHWt2-Q0inN0bQMXUXG%2FeyUUriMu0j5ZkFfAi%2FqxlWm0D6ATdwCL95PtIIQHmkCBuvs8qXVDDFky6q8Trh8vMw%3D%3D%3Ao8ZNquaJYFJWHfDCrm9zH8MGa9Jell5wENexYptlUz%2FiZPvDD8MHYk%2FX--Z22Ri0A2mN5QrwCOOUaJqmvk1xhNLdJsUIeHfbKemFRhLhKRw%3D; lastSeen=0; bkng=11UmFuZG9tSVYkc2RlIyh9YXSgTtYpR%2F1WOjMvuuinviEMkxKMKaR8RszOeejweDlwtP1u52V2KKHcuZWtAZXTCazM97dd71rXbb02cdIlBFgcQWZEsiJuYMTFSTHwgEN%2F4WK8rNDgBhcNIUGP6ch8GSLQSQolvOnV8yGkdtp5trbYww1Oad21qOwR5g%2BoBEpQZHaR3ehEvUWa75BXgEUL%2Fg%3D%3D',\n",
        "                    )\n",
        "  time.sleep(0.1)\n",
        "  response=session.get(url, headers=headers,params=payload) # Hit the url and get the response\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  # Get the number of properties loaded for the givn city\n",
        "  pages = soup.find_all('div',{'data-capla-component':'b-search-web-searchresults/HeaderDesktop'})\n",
        "  pages = pages[0].find_all('h1')[0].string\n",
        "  pages = int((pages.split(':')[1].strip()).split('properties')[0].strip())\n",
        "  \n",
        "  return pages # Return the number of properties loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yeLK2fv3N95n"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This function scrapes the Hotel details.\n",
        "Accepts 1 arguments:\n",
        "city_: (str) Name of the city you want the list of hotels for \n",
        "\n",
        "Returns the list of hotels for the given city\n",
        "'''\n",
        "def scrape_hotels_list(city_):\n",
        "  #properties_num = get_pages(city_)\n",
        "  hotel_list = []\n",
        "  # Run this loop using the number of properties (0, properties_num, 25) and 25 as offset\n",
        "  for i in range (0,201,25): # This was modified for each run to load data in chunks\n",
        "    url = 'https://www.booking.com/searchresults.en-us.html' # Set the URL\n",
        "    payload = set_payload_a(city=city_[0], dest=city_[1], aid=city_[2], sid=city_[3], offset=i)\n",
        "    #Set your cookie here\n",
        "    headers = set_header(cookie=\n",
        "                     'px_init=0; bkng_sso_session=e30; cors_js=1; OptanonConsent=consentCausedByGPCSignalStored=1676350216807&implicitConsentCountry=nonGDPR&implicitConsentDate=1676350578327; bkng_sso_ses=eyJib29raW5nX2dsb2JhbCI6W3siYSI6MSwiaCI6IlpaWXQzN1U2cWxmSG5zWlN6bkJmU3lacnZ0bndBN3IvZjJVd2lreGJVeEUifV19; bkng_sso_auth=CAEQARpWiuKYi1DOHJSuO4hRLUcoob8rBLZ39K9khJGFmTPO4onFDiuuzjOjOmcFBJGuerC/vcR6HYSE9ycFThhsbi9pV/XvIfCvie32L75iYUAamGzwavCmlNAiEMNjyhVSP6kdS4TMEjCNd6s=; BJS=-; _pxhd=kHWt2-Q0inN0bQMXUXG%2FeyUUriMu0j5ZkFfAi%2FqxlWm0D6ATdwCL95PtIIQHmkCBuvs8qXVDDFky6q8Trh8vMw%3D%3D%3Ao8ZNquaJYFJWHfDCrm9zH8MGa9Jell5wENexYptlUz%2FiZPvDD8MHYk%2FX--Z22Ri0A2mN5QrwCOOUaJqmvk1xhNLdJsUIeHfbKemFRhLhKRw%3D; lastSeen=0; bkng=11UmFuZG9tSVYkc2RlIyh9YXSgTtYpR%2F1WOjMvuuinviEMkxKMKaR8RszOeejweDlwtP1u52V2KKHcuZWtAZXTCazM97dd71rXbb02cdIlBFgcQWZEsiJuYMTFSTHwgEN%2F4WK8rNDgBhcNIUGP6ch8GSLQSQolvOnV8yGkdtp5trbYww1Oad21qOwR5g%2BoBEpQZHaR3ehEvUWa75BXgEUL%2Fg%3D%3D',\n",
        "                    )\n",
        "    time.sleep(0.1)\n",
        "    response=session.get(url, headers=headers,params=payload) # Hit the URL and load the response\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    items = soup.find_all('div',{'data-testid':'property-card'})\n",
        "\n",
        "    # Loop through each hotel property to scrape the data\n",
        "    for item in items:\n",
        "      try: # Scrape hotel name\n",
        "        name = item.find_all('div',{'data-testid':'title'})[0].string\n",
        "      except:\n",
        "        name = None\n",
        "      try: # Scrape hotel location\n",
        "        location = item.find_all('span',{'data-testid':'distance'})[0].string\n",
        "      except:\n",
        "        location = None\n",
        "      try: # Scrape hotel price\n",
        "        price = item.find_all('span',{'data-testid':'price-and-discounted-price'})[0].string\n",
        "      except:\n",
        "        price = None\n",
        "      try: # Scrape room type\n",
        "        room = item.find_all('div',{'data-testid':'recommended-units'})\n",
        "        room_type = room[0].find_all('span')[0].string\n",
        "      except:\n",
        "        room_type = None\n",
        "      try: # Scrape link of the hotel\n",
        "        link = (item.find_all('a',{'data-testid':'title-link'})[0]['href']).split('?')[0]\n",
        "        link = link.split('https://www.booking.com')[1]\n",
        "      except:\n",
        "        link = None\n",
        "      try: # Scrape review score, review category and total number of reviews for the hotel\n",
        "        rev_list = item.find_all('a',{'data-testid':'review-score-link'})\n",
        "        rev_list = rev_list[0].findAll('div')\n",
        "        review_score = rev_list[1].string\n",
        "        review_category = rev_list[3]['aria-label']\n",
        "        revs = rev_list[4].string\n",
        "      except:\n",
        "        list_rev = None\n",
        "        review_score = None\n",
        "        review_category = None\n",
        "        revs = None\n",
        "      flag = 0\n",
        "      # Check if the hotel is already scrapped, if yes skip it\n",
        "      if len(hotel_list) != 0:\n",
        "          for k in hotel_list:\n",
        "            if k['name'] == name:\n",
        "              flag = 1\n",
        "      if flag == 1:\n",
        "        continue\n",
        "      if revs != None:\n",
        "        # If number of reviews is not None, scrape reviews by calling the function\n",
        "        hotel_details = scrape_hotel(link,revs)\n",
        "        amenities = hotel_details[1]\n",
        "        ratings = hotel_details[2]\n",
        "        review_list = hotel_details[0]\n",
        "        if name is not None and link is not None:\n",
        "          # If hotel name and link are not none, add the hotel data into a dictionary and append it to a list\n",
        "          hotel_list.append({'name':name,\n",
        "                         'location':location,\n",
        "                         'price':price,\n",
        "                         'room_type':room_type,\n",
        "                         'link':link,\n",
        "                         'review_score':review_score,\n",
        "                         'review_category':review_category,\n",
        "                         'no_of_reviews':revs,\n",
        "                         'amenities':amenities,\n",
        "                         'ratings':ratings,\n",
        "                         'reviews_list':review_list\n",
        "                         })    \n",
        "  # Return hotel list\n",
        "  return hotel_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KG_1k3kOKM_",
        "outputId": "93e20117-1554-464f-9b77-882474c12f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:01<00:00,  8.79it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.82it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.21it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.35it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.90it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.94it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.74it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.42it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.07it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.71it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.08it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.05it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.16it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.03it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.50it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.04it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.20it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.87it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.08it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.21it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.59it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.48it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.83it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.01it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.08it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.14it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.62it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.69it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.67it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.00it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.05it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.62it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  3.77it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.06it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.12it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.04it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.93it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.65it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.68it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.96it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.78it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.35it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.88it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.64it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.67it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.02it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.01it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.59it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.03it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.64it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.99it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.93it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.98it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.81it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.18it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.89it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.70it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.97it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.22it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.65it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.96it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.03it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.06it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.09it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.94it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.61it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.41it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.04it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.04it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.69it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.76it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.45it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.04it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.09it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.90it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.04it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.12it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.60it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.22it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.30it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.00it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.24it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.09it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.23it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.21it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.59it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.31it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.33it/s]\n",
            "100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n",
            "100%|██████████| 10/10 [00:08<00:00,  1.24it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.09it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.23it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.30it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.93it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.83it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.18it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.86it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  7.17it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  3.97it/s]\n",
            "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  6.28it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  8.92it/s]\n"
          ]
        }
      ],
      "source": [
        "# Define cities to be scrapped\n",
        "cities = [['San Francisco', '20015732', '304142', 'abb6e0ad44a11fc8234d1e153cbab286'], \n",
        "          ['Los Angeles, California, United States', '20014181', '304142', 'abb6e0ad44a11fc8234d1e153cbab286'],\n",
        "          ['San Diego, California, United States', '20015725', '355028', 'abb6e0ad44a11fc8234d1e153cbab286'],\n",
        "          ['Sacramento, California, United States', '20015688', '355028', 'abb6e0ad44a11fc8234d1e153cbab286'],\n",
        "          ['Santa Cruz, california, United States', '20015797', '355028', 'abb6e0ad44a11fc8234d1e153cbab286']]\n",
        "\n",
        "final_hotel_list = {}\n",
        "hotel_list = scrape_hotels_list(cities[1]) # Scrape city 1 by 1 as the huge amount of daa is crashing the RAM\n",
        "\n",
        "# Add data into dictionary\n",
        "final_hotel_list['city_name'] = cities[1][0]\n",
        "final_hotel_list['hotel_list'] = hotel_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MaKfzjRFsSrA"
      },
      "outputs": [],
      "source": [
        "# Export data from dictionary into a JSON file\n",
        "# File name set as Scrape_data_<city name><num> num is 1,2,3... for each city\n",
        "with open('Scrape_data_LA1.json', 'w') as f: # Change JSON name for each city while loading data in chunks\n",
        "  json.dump(final_hotel_list, f, indent=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}